### 项目重构与设计要求

从外部加载的配置需要`config`来解析，并添加`runtime.Context`的字段，比如运行`./kubexm create cluster -f config.yaml`时，需要加载解析`config.yaml`，并初始化到`runtime.Context`。

- `pkg/common`: 放置一些常量, 比如 `common.KUBEXM=".kubexm"`, `common.TMP="/tmp/.kubexm"` 等等。
- `pkg/cache`: 是缓存 (pipelinecache, modulecache, taskcache, stepcache)。
- `pkg/util`: 是一些工具函数。

`runtime.builder`初始化的时候应该：
1.  生成`GenerateWorkDir`，即程序所在机器(即当前机器、本机)当前目录 `$(pwd)/.kubexm`。
2.  生成`HostWorkDir`，即程序所在机器(即当前机器、本机)当前目录 `$(pwd)/.kubexm/${hostname}`。

#### 文件与目录结构规划

- **生成的etcd根证书**: 放在 `workdir/.kubexm/${cluster_name}/certs/etcd/`
- **生成etcd的其他证书**: 放在 `workdir/.kubexm/${cluster_name}/certs/etcd/` 下面
- **下载的etcd等二进制文件**: 放在 `workdir/.kubexm/${cluster_name}/etcd/${etcd_version}/${arch}/` 下面
- **下载的docker、containerd等运行时文件**: 放在 `workdir/.kubexm/${cluster_name}/container_runtime/${container_runtime_name}/${container_runtime_version}/${arch}/` 下面
- **下载的kubelet、kubadm等kubernetes文件**: 放在 `workdir/.kubexm/${cluster_name}/kubernetes/${kubernetes_version}/${arch}/` 下面

#### 文件分发流程

1.  将证书从本机分发到所有etcd节点和master节点。
2.  将etcd分发到所有etcd节点。
3.  将运行时分发到所有节点。
4.  将kubernetes相关文件分发到所有节点。
5.  将registry或harbor分发到标记角色为`registry`的节点，然后启动registry或harbor。

---

### 项目重构指令

请读取我项目里面的markdown文件，理解项目的功能和架构，然后分层实现功能。遵循关注点分离原则和单一原则，遵循分层实现原则。 请读取我项目中的源码，了解现有项目的功能和架构。

**始终记得pipeline、module、task、step分层模型，绝不允许跨层调用。**

我这里的本地`workdir`指的是程序所在的节点，不是控制节点。程序所在的机器不属于集群的节点，我所有的包都下载到程序所在的节点。

**具体操作：**

- 对整个项目进行重构。
- 将 `pkg/step/intall_packages.go` 移动到 `pkg/step/common/install_packages.go`。
- 将 `pkg/task/install_nginx.go` 删除。
- 将 `pkg/pipeine/creaate_cluster_pipeline.go` 移动到 `pkg/pipeline/cluster/create.go`。
- 将 `pkg/runtime/runtime.go` 删除。
- 对 `pkg/step` 下面所有文件、包含子目录的文件进行重构。
- 对 `pkg/task` 下面所有文件、包含子目录的文件进行重构。
- 对 `pkg/module` 下面所有文件、包含子目录的文件进行重构。
- 对 `pkg/pipeline` 下面所有文件、包含子目录的文件进行重构。

`pkg/resources` 将二进制文件下载到本机(程序运行的机器，local), 放置在：

1.  `runtime.builder`初始化的时候应该生成`GenerateWorkDir`，即程序所在机器(即当前机器、本机)当前目录 `$(pwd)/.kubexm`。
2.  `runtime.builder`初始化的时候应该生成`HostWorkDir`，即程序所在机器(即当前机器、本机)当前目录 `$(pwd)/.kubexm/${hostname}`。
3.  生成的etcd根证书,放在`workdir/.kubexm/${cluster_name}/certs/etcd/`。
4.  生成etcd的其他证书,放在`workdir/.kubexm/${cluster_name}/certs/etcd/`下面。
5.  下载的etcd 等二进制文件放在`workdir/.kubexm/${cluster_name}/etcd/${etcd_version}/${arch}/`下面。
6.  下载的docker、containerd等运行时文件放在`workdir/.kubexm/${cluster_name}/container_runtime/${container_runtime_name}/${container_runtime_version}/${arch}/`下面。
7.  下载的kubelet、kubadm等kubernetes文件放在`workdir/.kubexm/${cluster_name}/kubernetes/${kubernetes_version}/${arch}/`下面。

然后需要分发。

---

### 配置文件示例 (`config.yaml`)

```yaml
apiVersion: kubexm.mensylisir.io/v1alpha1
kind: Cluster
metadata:
  name: mycluster
spec:
  hosts:
  # Assume that the default port for SSH is 22. Otherwise, add the port number after the IP address. 
  # If you install Kubernetes on ARM, add "arch: arm64". For example, {...user: ubuntu, password: Qcloud@123, arch: arm64}.
  - {name: node1, address: 172.16.0.2, internalAddress: "172.16.0.2,2022::2", port: 8022, user: ubuntu, password: "Qcloud@123"}
  # For default root user.
  # Kubekey will parse `labels` field and automatically label the node.
  - {name: node2, address: 172.16.0.3, internalAddress: "172.16.0.3,2022::3", password: "Qcloud@123", labels: {disk: SSD, role: backend}}
  # For password-less login with SSH keys.
  - {name: node3, address: 172.16.0.4, internalAddress: "172.16.0.4,2022::4", privateKeyPath: "~/.ssh/id_rsa"}
  - {name: node4, address: 172.16.0.5, internalAddress: "172.16.0.5,2022::4", privateKeyPath: "~/.ssh/id_rsa"}
  - {name: node5, address: 172.16.0.6, internalAddress: "172.16.0.6,2022::4", privateKeyPath: "~/.ssh/id_rsa"}
  - {name: node6, address: 172.16.0.7, internalAddress: "172.16.0.7,2022::4", privateKeyPath: "~/.ssh/id_rsa"}
  - {name: node7, address: 172.16.0.8, internalAddress: "172.16.0.8,2022::2", port: 8022, user: ubuntu, password: "Qcloud@123"}
  - {name: node8, address: 172.16.0.9, internalAddress: "172.16.0.9,2022::2", port: 8022, user: ubuntu, password: "Qcloud@123"}
  - {name: node10, address: 172.16.0.10, internalAddress: "172.16.0.10,2022::3", password: "Qcloud@123", labels: {disk: SSD, role: backend}}
  - {name: node11, address: 172.16.0.11, internalAddress: "172.16.0.11,2022::3", password: "Qcloud@123"}
  - {name: node12, address: 172.16.0.12, internalAddress: "172.16.0.12,2022::3", password: "Qcloud@123"}
  - {name: node13, address: 172.16.0.13, internalAddress: "172.16.0.13,2022::3", password: "Qcloud@123"}
  roleGroups:
    etcd:
    - node1 # All the nodes in your cluster that serve as the etcd nodes.
    - node2
    - node3
    master:
    - node4
    - node[5:6] # From node2 to node10. All the nodes in your cluster that serve as the master nodes.
    worker:
    - node7
    - node[8:10] # All the nodes in your cluster that serve as the worker nodes.
    ## Specify the node role as registry. Only one node can be set as registry. For more information check docs/registry.md
    registry:
    - node11
    loadbalancer: # 如果loadbalancer有节点，则需要在这两个节点上配置keepalived+haproxy或者keepalived+nginx作为负载均衡
    - node12
    - node13
    
  controlPlaneEndpoint:
    # External loadbalancer for apiservers. Support: kubexm，external [Default: ""]
    # 当externalLoadBalancer是kubexm时，hosts列表里loadbalancer必须有节点
    # 当externalLoadBalancer是external时,hosts列表不能有loadbalancer，使用用户自己搭建的负载均衡，或者其他已经存在的负载均衡
    # 当externalLoadBalancer是空时，不启用外部负载均衡
    externalLoadBalancer: kubexm
    # Internal loadbalancer for apiservers. Support: haproxy，nginx, kube-vip [Default: ""]
    # 当externalLoadBalancer不启用时，internalLoadbalancer必须启用
    # internalLoadbalancer为haproxy表示在每个worker节点部署haproxy的pod，使worker通过haproxy代理到kube-apiserver
    # internalLoadbalancer为nginx表示在每个worker节点部署nginx的pod，使worker通过nginx代理到kube-apiserver
    # internalLoadbalancer为kube-vip表示使用kube-vip代理kube-apiserver
    internalLoadbalancer: haproxy| nginx | kube-vip
    # Determines whether to use external dns to resolve the control-plane domain. 
    # If 'externalDNS' is set to 'true', the 'address' needs to be set to "".
    # 
    externalDNS: false  
    domain: lb.kubesphere.local
    # 如果使用External loadbalancer，则lb_address应该填vip
    # 如果使用internalLoadblancer in "kube-vip" mode,，则lb_address应该填vip
    lb_address: ""      
    port: 6443
  system:
    # The ntp servers of chrony.
    ntpServers:
      - time1.cloud.tencent.com # 表示给所有hosts里面的机器都要配置向这个链接同步时间
      - ntp.aliyun.com # 表示给所有hosts里面的机器都要配置向这个链接同步时间
      - node1 # 这个名字在hosts列表中 ，表示将这个机器作为ntpserver，你需要在他上面安装chrony并配置启用chronyd
    timezone: "Asia/Shanghai" # 在hosts列表的所有机器设置zone
    # Specify additional packages to be installed. The ISO file which is contained in the artifact is required.
    rpms:
      - nfs-utils
    # Specify additional packages to be installed. The ISO file which is contained in the artifact is required.
    debs: 
      - nfs-common
  kubernetes:
    # 有两个值，kubexm表示使用二进制部署kubernetes， kubeadm表示使用Kubeadm部署kubernetes
    type: kubexm
    #kubelet start arguments
    #kubeletArgs:
      # Directory path for managing kubelet files (volume mounts, etc).
    #  - --root-dir=/var/lib/kubelet
    version: v1.32.4
    # Optional extra Subject Alternative Names (SANs) to use for the API Server serving certificate. Can be both IP addresses and DNS names.
    apiserverCertExtraSans:  
      - 192.168.8.8
      - lb.kubespheredev.local
    # Container Runtime, support: containerd, cri-o, isula. [Default: docker]
    containerRuntime: 
      type: containerd
      version： 1.7.6
      containerd：
        version： 1.7.6
        registryMirrors: "aliyun.com"
        insecureRegistries: "aaa.com"
    clusterName: cluster.local
    # Whether to install a script which can automatically renew the Kubernetes control plane certificates. [Default: false]
    autoRenewCerts: true
    # masqueradeAll tells kube-proxy to SNAT everything if using the pure iptables proxy mode. [Default: false].
    masqueradeAll: false
    # maxPods is the number of Pods that can run on this Kubelet. [Default: 110]
    maxPods: 110
    # podPidsLimit is the maximum number of PIDs in any pod. [Default: 10000]
    podPidsLimit: 10000
    # The internal network node size allocation. This is the size allocated to each node on your network. [Default: 24]
    nodeCidrMaskSize: 24
    # Specify which proxy mode to use. [Default: ipvs]
    proxyMode: ipvs
    # enable featureGates, [Default: {"ExpandCSIVolumes":true,"RotateKubeletServerCertificate": true,"CSIStorageCapacity":true, "TTLAfterFinished":true}]
    featureGates: 
      CSIStorageCapacity: true
      ExpandCSIVolumes: true
      RotateKubeletServerCertificate: true
      TTLAfterFinished: true
    ## support kata and NFD
    # kata:
    #   enabled: true
    # nodeFeatureDiscovery
    #   enabled: true
    # additional kube-proxy configurations
    kubeProxyConfiguration:
      ipvs:
        # CIDR's to exclude when cleaning up IPVS rules.
        # necessary to put node cidr here when internalLoadbalancer=kube-vip and proxyMode=ipvs
        # refer to: https://github.com/kubesphere/kubekey/issues/1702
        excludeCIDRs:
          - 172.16.0.2/24
  etcd:
    # Specify the type of etcd used by the cluster. 可以取这几个值 [kubexm | kubeadm | external] [Default: kubexm]
    # kubexm表示二进制部署etcd
    # kubeadm表示使用kubeadm部署etcd
    # external表示已经有外部配置好的etcd
    type: kubexm  
    ## The following parameters need to be added only when the type is set to external.
    ## caFile, certFile and keyFile need not be set, if TLS authentication is not enabled for the existing etcd.
    # external:
    #   endpoints:
    #     - https://192.168.6.6:2379
    #   caFile: /pki/etcd/ca.crt
    #   certFile: /pki/etcd/etcd.crt
    #   keyFile: /pki/etcd/etcd.key
    dataDir: "/var/lib/etcd"
    # Time (in milliseconds) of a heartbeat interval.
    heartbeatInterval: 250
    # Time (in milliseconds) for an election to timeout. 
    electionTimeout: 5000
    # Number of committed transactions to trigger a snapshot to disk.
    snapshotCount: 10000
    # Auto compaction retention for mvcc key value store in hour. 0 means disable auto compaction.
    autoCompactionRetention: 8
    # Set level of detail for etcd exported metrics, specify 'extensive' to include histogram metrics.
    metrics: basic
    ## Etcd has a default of 2G for its space quota. If you put a value in etcd_memory_limit which is less than
    ## etcd_quota_backend_bytes, you may encounter out of memory terminations of the etcd cluster. Please check
    ## etcd documentation for more information.
    # 8G is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it.
    quotaBackendBytes: 2147483648 
    # Maximum client request size in bytes the server will accept.
    # etcd is designed to handle small key value pairs typical for metadata.
    # Larger requests will work, but may increase the latency of other requests
    maxRequestBytes: 1572864
    # Maximum number of snapshot files to retain (0 is unlimited)
    maxSnapshots: 5
    # Maximum number of wal files to retain (0 is unlimited)
    maxWals: 5
    # Configures log level. Only supports debug, info, warn, error, panic, or fatal.
    logLevel: info
  network:
    plugin: calico
    calico:
      ipipMode: Always  # IPIP Mode to use for the IPv4 POOL created at start up. If set to a value other than Never, vxlanMode should be set to "Never". [Always | CrossSubnet | Never] [Default: Always]
      vxlanMode: Never  # VXLAN Mode to use for the IPv4 POOL created at start up. If set to a value other than Never, ipipMode should be set to "Never". [Always | CrossSubnet | Never] [Default: Never]
      vethMTU: 0  # The maximum transmission unit (MTU) setting determines the largest packet size that can be transmitted through your network. By default, MTU is auto-detected. [Default: 0]
    kubePodsCIDR: 10.233.64.0/18,fd85:ee78:d8a6:8607::1:0000/112
    kubeServiceCIDR: 10.233.0.0/18,fd85:ee78:d8a6:8607::1000/116
    # 指定 ippool的blocksize
    ippool:
      blockSize: 26
  storage:
    openebs:
      basePath: /var/openebs/local # base path of the local PV provisioner
  # 表示要在registry机器上部署registry或harbor作为镜像仓库
  registry:
    privateRegistry: "dockerhub.kubexm.local" # 镜像仓库的域名
    namespaceOverride: "" # 镜像仓库的项目
    auths: # if docker add by `docker login`, if containerd append to `/etc/containerd/config.toml`
      "dockerhub.kubexm.local":
        username: "xxx"
        password: "***"
        skipTLSVerify: false # Allow contacting registries over HTTPS with failed TLS verification.
        plainHTTP: false # Allow contacting registries over HTTP.
        certsPath: "/etc/docker/certs.d/dockerhub.kubexm.local" # Use certificates at path (*.crt, *.cert, *.key) to connect to the registry.
    containerdDataDir: /var/lib/containerd
    dockerDataDir: /var/lib/docker
    registryDataDir: /mnt/registry
  addons: ["metallb", "longhorn"] # You can install cloud-native addons (Chart or YAML) by using this field.
   #  ## Optional hosts file content to append  to /etc/hosts file for all hosts.
  #host:
  #  192.168.1.100 bhy.example.com
  #dns:
  #  ## Optional hosts file content to coredns use as /etc/hosts file.
  #  dnsEtcHosts: |
  #    192.168.0.100 api.example.com
  #    192.168.0.200 ingress.example.com
  #  coredns:
  #    ## additionalConfigs adds any extra configuration to coredns
  #    additionalConfigs: |
  #      whoami
  #      log
  #    ## Array of optional external zones to coredns forward queries to. It's injected into coredns' config file before
  #    ## default kubernetes zone. Use it as an optimization for well-known zones and/or internal-only domains, i.e. VPN for internal networks (default is unset)
  #    externalZones:
  #    - zones:
  #      - example.com
  #      - example.io:1053
  #      nameservers:
  #      - 1.1.1.1
  #      - 2.2.2.2
  #      cache: 5
  #    - zones:
  #      - mycompany.local:4453
  #      nameservers:
  #      - 192.168.0.53
  #      cache: 10
  #    - zones:
  #      - mydomain.tld
  #      nameservers:
  #      - 10.233.0.3
  #      cache: 5
  #      rewrite:
  #      - name substring website.tld website.namespace.svc.cluster.local
  #    ## Rewrite plugin block to perform internal message rewriting.
  #    rewriteBlock: |
  #      rewrite stop {
  #        name regex (.*)\.my\.domain {1}.svc.cluster.local
  #        answer name (.*)\.svc\.cluster\.local {1}.my.domain
  #      }
  #    ## DNS servers to be added *after* the cluster DNS. These serve as backup
  #    ## DNS servers in early cluster deployment when no cluster DNS is available yet.
  #    upstreamDNSServers:
  #    - 8.8.8.8
  #    - 1.2.4.8
  #    - 114.114.114.114
  #  nodelocaldns:
  #    ## It's possible to extent the nodelocaldns' configuration by adding an array of external zones.
  #    externalZones:
  #    - zones:
  #      - example.com
  #      - example.io:1053
  #      nameservers:
  #      - 1.1.1.1
  #      - 2.2.2.2
  #      cache: 5
  #    - zones:
  #      - mycompany.local:4453
  #      nameservers:
  #      - 192.168.0.53
  #      cache: 10
  #    - zones:
  #      - mydomain.tld
  #      nameservers:
  #      - 10.233.0.3
  #      cache: 5
  #      rewrite:
  #      - name substring website.tld website.namespace.svc.cluster.local