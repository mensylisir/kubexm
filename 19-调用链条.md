### 调用关系全景图 (基于执行图/DAG模型 - 终极详细版)

#### **核心原则 (维持不变):**
*   **控制流与数据流**: 严格自上而下：`main` -> `Pipeline` -> `Module` -> `Task` -> `Step`。
*   **决策与执行分离**: 上四层负责“思考”和“计划”，`Engine` 和 `Runner` 负责“行动”。`Context` 则是贯穿一切的“信使”和“状态板”。

---

### **第一阶段：初始化与“世界感知” (程序启动时)**
此阶段与您的描述完全一致，因为它与计划的结构无关。

*   **`main()` (在 `cmd/kubexm/main.go`)**
    *   **职责**: 程序入口，解析命令行参数（使用 Cobra）。
    *   **调用**: 调用: 调用 cmd.Execute()。
        
*   ** cmd.Execute() (cmd/root.go)
    *   **职责**: 启动 Cobra 命令执行。
    *   **调用**: Cobra 框架根据用户输入的命令（如 kubexm cluster create）调用对应的 RunE 函数。

### **第二阶段：计划生成 - “图的构建” (由 create.go、delete.go等 发起)**
1. **createCmd.RunE (cmd/cluster/create.go)**
    - **职责**:
        - 解析命令标志（如 --file, --dry-run）。
        - 调用 runtime.NewRuntimeBuilder() 创建运行时环境（PipelineContext）。
        - 创建 pipeline 实例。
    - **调用**: **现在由 create.go 直接调用 pipeline.Plan()**。
2. **Pipeline.Plan(ctx) (pkg/pipeline/)**
    - **职责**: 作为最终的图组装者，汇集所有模块的规划结果，生成单一、完整的执行图。
    - **调用**:
        - 遍历其下的所有 Module。
        - 按顺序调用 module.Plan()。它可以将前一个模块的出口节点ID作为上下文信息传递给下一个模块，以建立跨模块依赖。
        - 收集所有 Module 返回的 ExecutionFragment（图的片段）。
        - 合并所有 Nodes 到最终的 ExecutionGraph 中，并解析/创建模块间的依赖关系。
    - **产出**: 返回一个完整的 plan.ExecutionGraph 给 create.go。
3. **Module.Plan(ctx)**
    - **职责**: 作为图的链接器，将内部任务的规划结果链接成一个更大的图片段。
    - **调用**:
        - 遍历其下的 Task，调用 task.IsRequired() 判断是否需要执行。
        - 若需要，则调用 task.Plan()。
        - **【核心】**: 接收 Task 返回的 ExecutionFragment，并将任务的出口节点 (ExitNodes) 与下一个任务的入口节点 (EntryNodes) 建立依赖关系。
    - **产出**: 返回一个链接好的 module.ExecutionFragment。
4. **Task.Plan(ctx)**
    - **【决策核心】**
    - **职责**: 基于配置和主机 Facts，决定需要执行哪些具体步骤 (Step)。
    - **调用**:
        - 不执行任何操作，而是调用 step.New...() 工厂函数来创建一系列 Step 实例。
        - 将 Step 和目标 Host 列表打包成 plan.ExecutionNode。
        - 为节点分配唯一ID，并定义节点间的内部依赖。
    - **产出**: 返回一个包含节点、入口和出口的 task.ExecutionFragment。

### **第三阶段：计划展示与确认 (在 create.go 中)**

*这个阶段是 Plan-Apply 模式的用户体验核心。*

1. **createCmd.RunE (回到 cmd/cluster/create.go)**
    - **职责**:
        - 接收从 pipeline.Plan() 返回的 ExecutionGraph。
        - **【展示】**: 调用一个辅助函数（如 ui.PrintGraph(graph)）将计划以友好的方式打印到控制台。
        - **【Dry-Run】**: 检查 --dry-run 标志。如果为 true，打印计划后直接成功返回，**不进入下一阶段**。
        - **【确认】**: （如果不是 dry-run）向用户显示确认提示。如果用户拒绝，则中止操作。


### **第四阶段：计划执行 - “图的调度” (由 create.go 发起)**

*此阶段由命令行层在用户确认后触发。*

1. **createCmd.RunE (继续在 cmd/cluster/create.go)**
    - **职责**: 如果用户确认执行，则调用 Pipeline 的执行方法。
    - **调用**: **pipeline.Run(ctx, executionGraph)**。注意，Run 方法现在接收 ExecutionGraph 作为参数。
2. **Pipeline.Run(ctx, graph) (pkg/pipeline/)**
    - **职责**: 协调执行过程。它现在非常简单，主要是一个委托者。
    - **调用**: 将收到的 graph 和 ctx 直接传递给 Engine。
        - ctx.Engine.Execute(ctx, graph)
3. **Engine.Execute(ctx, graph)**
    - **【执行总指挥 - 图调度器】**
    - **职责**:
        - 验证图的有效性（如无循环依赖）。
        - 使用**拓扑排序**和**并发工作池 (worker pool)** 的方式来调度和执行图中的节点。
        - **调度逻辑**:
            1. 计算所有节点的入度 (in-degree)。
            2. 将所有入度为 0 的节点放入“可执行队列”。
            3. Worker 从队列中取出节点并发执行。
            4. 节点执行成功后，Engine 找到其所有下游邻接节点，将其入度减 1。
            5. 如果下游节点的入度变为 0，则将其放入“可执行队列”。
        - **失败处理**: 如果节点失败，所有依赖它的下游节点都将被标记为 Skipped。
    - **调用**: 对于每个 ExecutionNode，Engine 会在所有指定的主机上并发地执行其关联的 Step。
    - **产出**: 返回一个包含所有节点执行结果的 GraphExecutionResult。
4. **Step.Run(ctx) -> Runner -> Connector -> 目标主机**
    - **【行动核心】**
    - 此微观执行流程保持不变，您的设计已非常完善。
    - Step 使用 Runner 服务来执行高级操作，Runner 拼装命令并使用 Connector 在目标主机上执行。

*   **`Step.Run()` -> `Runner` 服务 -> `Connector` -> 目标主机**
    *   **【行动核心】** 这一部分的微观执行流程与您的描述完全相同，是整个架构中最稳定的部分。
    *   **职责**: `Step.Run()` 是具体操作的实现者。
    *   **调用**:
        *   从 `StepContext` 中调用 `GetRunner()` 获取无状态的 `Runner` 服务。
        *   调用 `GetConnectorForHost(host)` 和 `GetHostFacts(host)`。
        *   将 `Connector` 和 `Facts` 作为参数，调用 `Runner` 服务提供的高级方法（例如：`runnerSvc.InstallPackages(...)`）。
    *   **`Runner` 服务 -> `Connector`**: `Runner` 内部根据 `Facts` 拼装出最终命令，然后调用 `conn.Exec()`。
    *   **`Connector` -> 目标主机**: `Connector` 最终负责通过 SSH 或本地 `os/exec` 来执行命令，并将结果（stdout, stderr, exitCode, error）返回。