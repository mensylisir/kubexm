调用关系全景图 (基于执行图/DAG模型 - 终极详细版)
核心原则 (维持不变):
控制流与数据流: 严格自上而下：main -> Pipeline -> Module -> Task -> Step。
决策与执行分离: 上四层负责“思考”和“计划”，Engine 和 Runner 负责“行动”。Context 则是贯穿一切的“信使”和“状态板”。
第一阶段：初始化与“世界感知” (程序启动时)
此阶段与您的描述完全一致，因为它与计划的结构无关。
main() (在 cmd/installer/main.go)
职责: 程序入口，解析命令行参数（使用 Cobra）。
调用: 调用 runtime.NewRuntimeBuilder(configFile).Build() 来创建整个应用的运行时环境。这是整个流程的“创世”步骤。
关键: 使用 defer cleanup() 确保程序退出时，所有资源（如连接池）被正确释放。
RuntimeBuilder.Build() (在 pkg/runtime/builder.go)
职责: 一个重量级的构造函数，负责创建和组装所有核心服务和状态。
调用:
调用 config.ParseFromFile() 读取并解析 cluster.yaml。
调用 runner.New() 创建一个无状态的 Runner 服务实例。
调用 engine.NewExecutor() 创建一个图调度器的 Engine 服务实例。
调用 connector.NewConnectionPool() 创建一个SSH连接池。
【核心】: 并发地初始化所有主机。这包括配置文件中定义的所有远程主机，以及一个程序化创建的、代表本地控制节点的特殊主机（使用 LocalConnector）。对每个主机执行以下操作：
调用 connector.NewSSHConnector() 或 connector.NewLocalConnector() 创建一个 Connector 实例。
调用 connector.Connect() 建立连接。
调用 runnerSvc.GatherFacts(conn)，使用 Runner 服务来收集该主机的所有事实（Facts）。
将每个主机的 Host 配置、已连接的 Connector 和收集到的 Facts 打包成一个 HostRuntime 对象。
产出: 返回一个巨大的、包含所有状态的智能 Context，以及一个 cleanup 函数。这个 Context 里有 Logger, Engine, Runner 服务，以及一个包含所有主机（远程+本地）的 map 形式的 HostRuntimes。
第二阶段：计划生成 - “图的构建” (Dry-Run 的核心)
此阶段是与线性模型的核心区别，从“合并列表”演变为“链接图”。
main() -> Pipeline.Run() (在 pkg/pipeline/)
main 函数调用 pipeline.Run()，并将智能 Context 传入。
Pipeline.Run() -> Pipeline.Plan()
Pipeline.Run() 的第一步是调用自身的 Plan() 方法。
Plan() 方法接收一个受限的 PipelineContext。
Pipeline.Plan() -> Module.Plan()
职责: Pipeline 作为最终的图组装者，负责将所有模块的规划结果汇集成一个单一的、可执行的最终图。
调用:
Pipeline.Plan() 遍历其下的所有 Module。
它按顺序调用 module.Plan()。在调用下一个模块的 Plan 时，它可能会将上一个模块的出口节点ID作为参数或上下文信息传递下去，以便实现跨模块的依赖链接。
它收集所有 Module 返回的 ExecutionFragment（图的片段），并将所有 Nodes 合并到最终的 ExecutionGraph 中。
在合并过程中，它负责创建模块之间的依赖关系。
Module.Plan() -> Task.Plan()
职责: Module 作为图的链接器，负责将内部多个任务的规划结果，按逻辑顺序链接成一个更大的图片段。
调用:
Module.Plan() 遍历其下的所有 Task。
它会先调用 task.IsRequired() 判断此任务是否需要执行。
如果需要，它调用 task.Plan()，并传递一个受限的 TaskContext。
【核心】: 它接收每个 Task 返回的 ExecutionFragment。然后，它执行链接操作：将前一个任务的 ExitNodes（出口节点），作为当前任务 EntryNodes（入口节点）的依赖。这是通过修改当前任务入口节点的 Dependencies 字段来实现的。
Task.Plan() -> step.New...()
【决策核心】 Task.Plan() 作为子图构建器，是“思考”的终点。
职责:
从 TaskContext 中调用 GetClusterConfig() 获取用户配置，调用 GetHostsByRole() 确定操作的目标主机（包括特殊的control-node）。
基于配置和 Facts 进行决策。
不执行任何操作，而是调用 step.New...() 工厂函数来创建一系列 Step 实例。
调用:
它将 Step 和对应的 Host 列表打包成 plan.ExecutionNode。
为每个节点分配一个唯一的 NodeID（例如，"download-etcd-v3.5.4"）。
定义节点之间的内部依赖关系。例如，upload-etcd 节点的 Dependencies 字段会被设置为 [ "download-etcd-v3.5.4" ]。
产出: 生成一个 task.ExecutionFragment 对象。这个对象包含：
一个 map，包含此任务创建的所有 ExecutionNode。
EntryNodes: 此子图中没有内部依赖的节点ID列表。
ExitNodes: 此子图中没有被内部其他节点依赖的节点ID列表。
计划聚合 (演变为 "图的组装"):
ExecutionFragment 被逐层返回、链接和合并：Task -> Module -> Pipeline。
最终，Pipeline.Plan() 返回一个包含了所有节点和它们之间复杂依赖关系的、完整的 plan.ExecutionGraph。
第三阶段：计划执行 - “图的调度”
Pipeline.Run() -> Engine.Execute()
Pipeline.Run() 在拿到最终的 ExecutionGraph 后，将其和完整的 Context 一起交给 Engine 服务。
调用: ctx.Engine.Execute(ctx, executionGraph, dryRun)。
Engine.Execute() -> Step.Precheck() & Step.Run()
【执行总指挥 - 图调度器】 Engine 的行为与线性模型完全不同。
职责:
验证: 首先调用 graph.Validate() 确保图中没有循环依赖。
初始化调度:
构建一个依赖关系图（例如，邻接表）。
计算每个节点的“入度”（in-degree），即它所依赖的节点数量。
调度循环:
将所有入度为 0 的节点放入一个并发安全的“可执行队列”。
启动一个固定大小的 worker 池，每个 worker 都是一个goroutine。
worker 从队列中取出 ExecutionNode 并发执行。对于每个节点，它会在所有指定的 Host 上并发地执行 Step。
生命周期: 在单个主机上执行 Step 时，生命周期不变：首先调用 step.Precheck()，如果需要则调用 step.Run()，如果失败则调用 step.Rollback()。
【核心】: 当一个节点成功执行完毕后，Engine 会找到所有依赖于它的下游节点，并将它们的入度减 1。如果某个下游节点的入度变为 0，则将其放入可执行队列。
失败处理: 如果一个节点失败，所有直接或间接依赖于它的下游节点都将被标记为 Skipped，并且不会被执行。
产出: Engine 在执行过程中，实时地收集每个节点的 NodeResult，最终聚合成一个完整的 GraphExecutionResult。
Step.Run() -> Runner 服务 -> Connector -> 目标主机
【行动核心】 这一部分的微观执行流程与您的描述完全相同，是整个架构中最稳定的部分。
职责: Step.Run() 是具体操作的实现者。
调用:
从 StepContext 中调用 GetRunner() 获取无状态的 Runner 服务。
调用 GetConnectorForHost(host) 和 GetHostFacts(host)。
将 Connector 和 Facts 作为参数，调用 Runner 服务提供的高级方法（例如：runnerSvc.InstallPackages(...)）。
Runner 服务 -> Connector: Runner 内部根据 Facts 拼装出最终命令，然后调用 conn.Exec()。
Connector -> 目标主机: Connector 最终负责通过 SSH 或本地 os/exec 来执行命令，并将结果（stdout, stderr, exitCode, error）返回。