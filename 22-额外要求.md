对整个项目进行重构
将pkg/step/intall_packages.go移动到pkg/step/common/install_packages.go
将pkg/task/install_nginx.go删除
将pkg/pipeine/creaate_cluster_pipeline.go移动到pkg/cluster/create.go
将pkg/runtime/runtime.go删除
对pkg/step下面所有文件、包含子目录的文件进行重构
对pkg/task下面所有文件、包含子目录的文件进行重构
对pkg/module下面所有文件、包含子目录的文件进行重构
对pkg/pipeline下面所有文件、包含子目录的文件进行重构
按顺序读取我项目里面的markdown文件，扫描整个项目的代码，对整个项目进行重构



你先扫描整个项目，将整个项目列出来(这个非常重要，要首先执行)，然后读取我项目里面的markdown文件，理解项目的功能和架构，然后分层实现功能。遵循关注点分离原则和单一原则，遵循分层实现原则。 请读取我项目中的源码，了解现有项目的功能和架构 始终记得pipeline、module、task、step分层模型，绝不允许跨层调用 对整个项目进行重构 将pkg/step/intall_packages.go移动到pkg/step/common/install_packages.go 将pkg/task/install_nginx.go删除 将pkg/pipeine/creaate_cluster_pipeline.go移动到pkg/pipeline/cluster/create.go 将pkg/runtime/runtime.go删除 对pkg/step下面所有文件、包含子目录的文件进行重构 对pkg/task下面所有文件、包含子目录的文件进行重构 对pkg/module下面所有文件、包含子目录的文件进行重构 对pkg/pipeline下面所有文件、包含子目录的文件进行重构 按顺序读取我项目里面的markdown文件，扫描整个项目的代码，对整个项目进行重构，自底向上对项目进行重构，
另外，pkg/resources将二进制文件下载到本机(程序运行的机器，local),放置在
1. runtime.builder初始化的时候应该生成GenerateWorkDir，即程序所在机器(即当前机器、本机)当前目录 $(pwd)/.kubexm
2. runtime.builder初始化的时候应该生成HostWorkDir，即程序所在机器(即当前机器、本机)当前目录 $(pwd)/.kubexm/${hostname}
3. 生成的etcd根证书,放在workdir/.kubexm/${cluster_name}/certs/etcd/
4. 生成etcd的其他证书,放在workdir/.kubexm/${cluster_name}/certs/etcd/下面
5. 下载的etcd 等二进制文件放在workdir/.kubexm/${cluster_name}/etcd/${etcd_version}/${arch}/下面
6. 下载的docker、containerd等运行时文件放在workdir/.kubexm/${cluster_name}/container_runtime/${container_runtime_name}/${container_runtime_version}/${arch}/下面
7. 下载的kubelet、kubadm等kubernetes文件放在workdir/.kubexm/${cluster_name}/kubernetes/${kubernetes_version}/${arch}/下面
   然后需要分发
   按照下列要求丰富step
1. download_etcd的step, 已经被pkg/resource代替
   解释: 正如您所说，在现代的自动化工具（如 KubeKey）中，下载操作被抽象化了。pkg/resource 这样的包通常会处理各种资源的下载、校验（checksum）和缓存。它不仅仅是下载 etcd，还可以下载 CNI 插件、kubelet 等其他二进制文件。这是一个更通用、更可复用的模块化设计。
2. 分发etcd包到etcd节点的step？
   解释: 这是安装过程的第一步。自动化工具会通过 SSH（通常使用 scp 或 sftp）将已经下载好的 etcd 二进制压缩包（如 etcd-v3.5.x-linux-amd64.tar.gz）从部署节点（或中控机）分发到每一个指定的 etcd 节点上，通常放在一个临时目录，如 /tmp。
3. 解压etcd包的step
   解释: 在每个 etcd 节点上，通过 SSH 执行 tar -zxvf 命令，将上一步分发的压缩包解压。解压后会得到一个目录，里面包含了 etcd 和 etcdctl 两个核心的二进制文件。
4. 将etcd的二进制复制到/usr/local/bin的step？
   解释: 为了让系统能够直接执行 etcd 和 etcdctl 命令，需要将这两个二进制文件从解压目录移动或复制到系统的 PATH 路径下，/usr/local/bin 是一个非常标准和推荐的位置。命令通常是 mv etcd /usr/local/bin/ 和 mv etcdctl /usr/local/bin/。
5. 将etcd证书分发到etcd节点和master节点的step?
   解释: 这是配置安全通信的关键步骤。
   分发到 etcd 节点: 每个 etcd 节点需要自己的服务器证书 (server.pem, server-key.pem) 和用于集群内部通信的对等证书 (peer.pem, peer-key.pem)，以及根证书 (ca.pem)。这些文件通常被放置在 /etc/etcd/pki 目录下。
   分发到 master 节点: Kubernetes 的 kube-apiserver 需要作为客户端连接 etcd 集群。因此，需要将根证书 (ca.pem) 和专门为 apiserver 生成的客户端证书 (apiserver-etcd-client.pem, apiserver-etcd-client-key.pem) 分发到所有 master 节点上。
6. 生成etcd配置文件的step？
   解释: 这一步是为 etcd 服务创建配置文件。在较新的版本中，通常使用 YAML 格式的配置文件（如 /etc/etcd/etcd.yaml）。这个文件是通过模板生成的，里面包含了节点的名称、IP 地址、数据目录、证书路径、集群成员列表 (initial-cluster) 等关键信息。自动化工具会根据每个节点的角色和 IP 动态渲染这个模板。
7. 生成etcd service的step？
   解释: 为了让 systemd 能够管理 etcd 服务，需要创建一个 service unit 文件，通常是 /etc/systemd/system/etcd.service。这个文件也通常由模板生成，定义了服务的描述、启动命令（ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.yaml）、重启策略等。
8. 启动第一台etcd的step
   解释: 这是集群引导（Bootstrap）的关键。在第一台 etcd 节点上，其配置文件中的 initial-cluster-state 必须设置为 new。这告诉 etcd 实例：“你要创建一个全新的集群，你是第一个成员”。启动这台节点后，集群就正式形成了（虽然只有一个成员）。
9. 将其他etcd节点加入到etcd集群的step？
   解释: 对于第二台及以后的 etcd 节点，其配置文件中的 initial-cluster-state 必须设置为 existing。这告诉 etcd 实例：“你要加入一个已经存在的集群”。启动这些节点后，它们会根据 initial-cluster 配置找到第一个节点并加入集群。
10. systemctl daemon-reload的step？
    解释: 在创建或修改了 etcd.service 文件（步骤 7）之后，必须执行此命令，通知 systemd 重新加载其配置，否则 systemd 不会识别到新的或变更的 etcd 服务。
11. systemctl start etcd的step?
    解释: 启动 etcd 服务。通常在集群初始化（步骤 8, 9）时使用。
12. systemctl restart etcd的step？
    解释: 重启 etcd 服务。通常在修改了配置文件（如 etcd.yaml）后需要执行此操作，以使新配置生效。（您写的 ecd 应该是 etcd 的笔误）。
13. systemctl stop etcd的step？
    解释: 停止 etcd 服务。在进行维护、备份或清理前可能会用到。
14. systemctl enable etcd的step
    解释: 设置 etcd 服务为开机自启动。这是一个非常重要的步骤，确保在节点重启后 etcd 服务能自动恢复。
15. systemctl disable etcd的step？
    解释: 取消 etcd 服务的开机自启动。在准备永久移除节点或废弃集群时使用。
16. 删除etcd的step？主要删除二进制文件
    解释: 这是清理步骤的一部分。通过执行 rm /usr/local/bin/etcd /usr/local/bin/etcdctl 来删除二进制文件。
17. 清理etcd的step，删除etcd的配置和service文件
    解释: 这是更彻底的清理。除了二进制文件，还需要删除配置文件、证书和 service 文件。例如：rm -rf /etc/etcd/ 和 rm /etc/systemd/system/etcd.service。
18. 备份etcd的step？
    解释: 灾备的关键步骤。通常使用 etcdctl 命令进行快照备份。
    Generated bash
    ETCDCTL_API=3 etcdctl snapshot save /path/to/backup/snapshot.db \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/etcd/pki/ca.pem \
    --cert=/etc/etcd/pki/server.pem \
    --key=/etc/etcd/pki/server-key.pem
    Use code with caution.
    Bash
19. 删除etcd数据目录的step？
    解释: 这是最危险的清理操作，会彻底删除集群的所有数据。通常在重建集群或恢复数据之前执行。数据目录在 etcd.yaml 中由 data-dir 指定，默认为 /var/lib/etcd。命令为 rm -rf /var/lib/etcd。
20. 恢复etcd的step？
    解释: 从备份快照中恢复数据。这个过程比较复杂，通常包括：
    停止所有 etcd 节点上的 etcd 服务和 kube-apiserver。
    清理所有 etcd 节点上的旧数据目录（步骤 19）。
    在一个节点上使用 etcdctl snapshot restore 命令恢复数据到一个新的数据目录。
    修改所有 etcd 节点的 etcd.service 或 etcd.yaml，确保 initial-cluster 配置正确，并指向新的数据目录。
    启动所有 etcd 服务。
    启动 kube-apiserver。
    etcd的步骤全吗？补充的步骤和考虑点
    您列出的步骤已经非常非常完整了，覆盖了从部署到运维再到清理的全过程。如果要追求生产级别的完美，可以补充以下几个方面：
1. 前置检查 (Pre-flight Checks)
   节点连通性检查: 确保中控机可以 SSH 到所有目标节点。
   端口检查: 确保 etcd 节点之间的 2379 (client) 和 2380 (peer) 端口是互通的，没有被防火墙阻挡。
   系统时间同步: 检查所有 etcd 节点是否都配置了 NTP 服务，并确保时间同步。时间不同步是分布式系统的灾难。
   主机名和/etc/hosts检查: 确保主机名解析正确。
2. 健康检查与验证 (Health Check & Verification)
   集群健康检查: 在集群启动后，执行 etcdctl endpoint health --cluster 来验证所有成员是否都健康。
   成员列表验证: 执行 etcdctl member list 来确认所有预期的节点都已成功加入集群。
3. 集群运维操作 (Cluster Operations)
   添加新成员 (Add Member): 当集群已经运行时，添加一个全新的节点。这需要使用 etcdctl member add 命令，而不仅仅是修改配置文件启动。
   移除成员 (Remove Member): 从正在运行的集群中优雅地移除一个节点。这需要先执行 etcdctl member remove <MEMBER_ID>，然后再去关闭并清理该节点。直接关闭节点而不移除，会导致集群性能下降。
   升级集群 (Upgrade): 对 etcd 版本进行滚动升级。这通常需要逐个节点停止旧版本、替换二进制文件、然后启动新版本，并在此过程中持续监控集群健康。
4. 监控与告警 (Monitoring & Alerting)
   Metrics 暴露: etcd 原生支持 Prometheus 格式的监控指标。需要在监控系统中配置抓取 etcd 的 metrics endpoint。
   关键告警配置:
   etcd_server_has_leader: 检查集群是否有 Leader。如果没有，是严重故障。
   etcd_mvcc_db_total_size_in_bytes: 监控数据库大小，防止超出配额。
   etcd_server_leader_changes_seen_total: Leader 切换频率，过于频繁表明网络不稳定。
   grpc_server_handled_total: 监控请求延迟（p99 latency）。
5. 日常维护 (Routine Maintenance)
   碎片整理 (Defragmentation): 定期（例如每周或每月）对 etcd 数据进行碎片整理，回收空间，提高性能。使用 etcdctl defrag --cluster。
   自动压缩 (Compaction): etcd 会保留历史版本，需要配置自动压缩策略 (--auto-compaction-retention) 来清理旧版本，防止数据库无限增长。
   结论
   您的清单已经覆盖了 95% 的核心操作，对于一次性的部署和基本的管理来说是完全足够的。我补充的几点（前置检查、健康检查、动态成员管理、监控和维护）则更多地是从长期、稳定、生产级运维的角度出发，使整个流程更加健壮和完善。
   总而言之，您对 etcd 的理解已经非常深入和系统了！


containerd 部署与管理步骤详解
containerd 的安装涉及到它自身、一个OCI运行时（通常是runc）以及CNI插件。
1. 前置准备：下载相关包 (由 pkg/resource 等模块处理)
   下载 containerd: 下载 containerd 的二进制压缩包，如 containerd-1.7.x-linux-amd64.tar.gz。
   下载 runc: runc 是 containerd 默认使用的 OCI 运行时，需要单独下载其二进制文件，如 runc.amd64。
   下载 CNI 插件: Kubernetes 需要 CNI (Container Network Interface) 插件来配置 Pod 网络。需要下载官方的插件包，如 cni-plugins-linux-amd64-v1.x.x.tgz。
2. 分发软件包到所有节点 (Master 和 Worker)
   解释: 与 etcd 不同，containerd 需要被安装在所有将要运行 Pod 的节点上，这通常包括所有的 Master 节点和 Worker 节点。
   动作: 通过 scp 或类似工具，将上述三个下载好的文件 (containerd 包, runc 二进制, cni-plugins 包) 分发到所有节点的 /tmp 等临时目录。
3. 安装二进制文件
   解释: 这是将所有组件解压并放置到系统标准路径的组合步骤。
   动作:
   安装 containerd:
   mkdir -p /usr/local/bin
   tar -zxvf containerd-*.tar.gz -C /usr/local/ (它会把 bin/ 目录下的 containerd, ctr 等解压到 /usr/local/bin/)
   安装 runc:
   install -m 755 runc.amd64 /usr/local/sbin/runc (install 命令可以同时复制文件并设置权限)
   安装 CNI 插件:
   mkdir -p /opt/cni/bin
   tar -zxvf cni-plugins-*.tgz -C /opt/cni/bin/
4. 生成并配置 containerd 配置文件
   解释: 这是配置 containerd 行为的核心步骤，尤其需要为 Kubernetes 进行适配。
   动作:
   创建配置目录: mkdir -p /etc/containerd
   生成默认配置文件: containerd config default > /etc/containerd/config.toml
   修改配置文件 (/etc/containerd/config.toml): 这是最关键的一步！
   启用 Systemd Cgroup Driver: 找到 [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options] 部分，将其中的 SystemdCgroup 改为 true。这为了让 containerd 和 kubelet 的 cgroup 管理方式保持一致。
   Generated toml
   [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
   SystemdCgroup = true
   Use code with caution.
   Toml
   配置 Sandbox (pause) 镜像: 确保 sandbox_image 指向一个可访问的 pause 镜像地址。如果国内访问不畅，可以换成国内镜像源。
   Generated toml
   [plugins."io.containerd.grpc.v1.cri"]
   sandbox_image = "registry.k8s.io/pause:3.9"
# 或者 "registry.aliyuncs.com/google_containers/pause:3.9"
Use code with caution.
Toml
(可选但强烈推荐) 配置镜像加速器: 为了加速拉取 docker.io 等仓库的镜像，需要配置 registry mirrors。
Generated toml
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
endpoint = ["https://<your-mirror-address>"]
Use code with caution.
Toml
5. 生成 containerd 的 systemd Service 文件
   解释: 创建一个 service unit 文件，让 systemd 管理 containerd 服务。
   动作: 创建 /etc/systemd/system/containerd.service 文件，内容如下：
   Generated ini
   [Unit]
   Description=containerd container runtime
   Documentation=https://containerd.io
   After=network.target local-fs.target

[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd
Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# a huge number of tasks - significantly larger than an OS default
TasksMax=infinity
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
Use code with caution.
Ini
6. systemctl 相关操作
   systemctl daemon-reload: 在创建 containerd.service 文件后，必须执行此命令重载 systemd 配置。
   systemctl start containerd: 启动 containerd 服务。
   systemctl restart containerd: 重启服务，通常在修改 config.toml 后执行。
   systemctl stop containerd: 停止服务。
   systemctl enable containerd: 设置服务开机自启。通常和 start 一起使用：systemctl enable --now containerd。
   systemctl disable containerd: 取消开机自启。
   systemctl status containerd: 查看服务状态，检查是否 active (running)。
7. 验证 containerd 是否正常工作
   解释: 使用 crictl 工具来验证 containerd 的 CRI 接口是否正常。crictl 是专门为 Kubernetes CRI 规范设计的客户端工具。
   动作:
   配置 crictl: 默认 crictl 会连接 containerd 的 socket，如果不是默认路径，需要配置 /etc/crictl.yaml。
   执行检查命令: crictl info 或 crictl ps。如果命令成功执行并返回信息，说明 CRI 接口工作正常。
8. 清理 containerd (卸载)
   解释: 从系统中移除 containerd 及其相关组件。
   动作:
   停止并禁用服务: systemctl stop containerd && systemctl disable containerd。
   删除二进制文件:
   rm /usr/local/bin/containerd* /usr/local/bin/ctr
   rm /usr/local/sbin/runc
   rm -rf /opt/cni/bin
   删除配置文件和服务文件:
   rm -rf /etc/containerd/
   rm -rf /etc/cni/net.d/ (CNI 插件的配置文件目录)
   rm /etc/systemd/system/containerd.service
   执行 systemctl daemon-reload。
9. 清理 containerd 的数据 (危险操作)
   解释: 此步骤会删除所有被 containerd 管理的容器镜像、容器快照等数据。
   动作: rm -rf /var/lib/containerd
   containerd的步骤全吗？补充的步骤和考虑点
   您的思路非常清晰，以上步骤已构成一个完整的部署和清理流程。以下是一些生产环境中非常重要的补充点：
1. 系统前置检查 (Prerequisites / Pre-flight Checks)
   加载内核模块: 确保 overlay 和 br_netfilter 模块已加载。可以通过创建 /etc/modules-load.d/k8s.conf 文件来使其永久生效。
   配置内核参数 (sysctl): 确保网络桥接时能处理 IPv4/IPv6 流量，通常在 /etc/sysctl.d/k8s.conf 中配置 net.bridge.bridge-nf-call-iptables = 1 和 net.ipv4.ip_forward = 1，然后执行 sysctl --system。
2. 与 Kubelet 的集成
   最重要的一步: containerd 的所有配置都是为了给 kubelet 服务的。你必须配置 kubelet，告诉它使用 containerd 作为容器运行时。
   动作: 在 kubelet 的启动参数中，必须指定：
   --container-runtime-endpoint=unix:///run/containerd/containerd.sock
   --cgroup-driver=systemd (必须与 containerd 中 SystemdCgroup = true 的配置保持一致)
3. 镜像管理
   预拉取镜像 (Pre-pulling): 在集群安装时，自动化工具通常会提前使用 crictl pull <image> 命令将 Kubernetes 核心组件（kube-apiserver, kube-scheduler, pause 等）的镜像拉取到每个节点，以加速集群启动。
   镜像清理: containerd 有内置的垃圾回收 (Garbage Collection) 机制来清理未使用的镜像，但也可以手动使用 crictl rmi <image-id> 来删除镜像。


Docker 和 cri-dockerd 部署与管理步骤详解
核心概念
Docker Engine: 强大的容器引擎，但它的 API 并不符合 Kubernetes 的 CRI (Container Runtime Interface) 规范。
cri-dockerd: 一个独立的开源项目，它扮演一个“桥梁”或“翻译官”的角色。它的一端提供符合 CRI 规范的 gRPC 接口给 kubelet 调用，另一端则将这些调用翻译成 Docker Engine 的 API 请求。
流程: kubelet → cri-dockerd (CRI gRPC) → dockerd (Docker API)
第 I 部分：安装和配置 Docker Engine
1. 分发和安装 Docker (所有 Master 和 Worker 节点)
   解释: 这是在节点上安装标准的 Docker 引擎。通常使用操作系统的包管理器来完成。
   动作 (以 Ubuntu/Debian 为例):
   卸载旧版本: apt-get remove docker docker-engine docker.io containerd runc
   设置 Docker 的 apt 仓库。
   安装 Docker Engine: apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin (注意：即使安装了 containerd.io 包，我们在这里也不直接使用它的 CRI 功能)。
2. 生成和配置 Docker 守护进程文件 (daemon.json)
   解释: 这是极其关键的一步，必须正确配置 Docker 以便与 Kubernetes 和 cri-dockerd 协同工作。
   动作:
   创建或编辑文件 /etc/docker/daemon.json。
   写入以下核心配置：
   Generated json
   {
   "exec-opts": ["native.cgroupdriver=systemd"],
   "log-driver": "json-file",
   "log-opts": {
   "max-size": "100m"
   },
   "storage-driver": "overlay2",
   "registry-mirrors": ["https://<your-mirror-address>"]
   }
   Use code with caution.
   Json
   "exec-opts": ["native.cgroupdriver=systemd"]: 必须设置！这告诉 Docker 使用 systemd 作为 cgroup 驱动，以便与 kubelet 的 cgroup 管理方式保持一致。这是最常见的出错点。
   "registry-mirrors": (强烈推荐) 配置国内镜像加速器，加速镜像拉取。
3. Docker 的 systemctl 相关操作
   systemctl daemon-reload: 如果修改了 daemon.json 或 systemd service 文件，需要执行。
   systemctl restart docker: 重启 Docker 使新配置生效。
   systemctl enable docker: 设置 Docker 开机自启。
   systemctl start docker: 启动 Docker 服务。
   systemctl stop docker: 停止 Docker 服务。
   第 II 部分：安装和配置 cri-dockerd
1. 下载和分发 cri-dockerd (所有 Master 和 Worker 节点)
   解释: cri-dockerd 是一个独立的二进制文件，需要从其 GitHub Releases 页面下载。
   动作:
   从 https://github.com/Mirantis/cri-dockerd/releases 下载对应的版本，例如 cri-dockerd-0.3.x.amd64.tgz。
   通过 scp 等工具将其分发到所有节点的 /tmp 目录。
2. 安装 cri-dockerd 二进制文件和 systemd 服务
   解释: 解压软件包，并将二进制文件和 service unit 文件放到正确的位置。
   动作:
   解压: tar -zxvf cri-dockerd-*.tgz
   复制二进制文件: mv cri-dockerd/cri-dockerd /usr/local/bin/
   复制 systemd 文件:
   mv cri-dockerd/packaging/systemd/* /etc/systemd/system/
   这会复制 cri-dockerd.service 和 cri-dockerd.socket 两个文件。
3. (可选) 修改 cri-dockerd.service 文件
   解释: 通常默认配置即可工作，但有时需要调整。
   动作:
   打开 /etc/systemd/system/cri-dockerd.service。
   检查 ExecStart 行: ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd://
   --container-runtime-endpoint 参数告诉 cri-dockerd 如何连接到 Docker Engine。fd:// 或 unix:///var/run/docker.sock 都是常见的配置。默认通常无需修改。
4. cri-dockerd 的 systemctl 相关操作
   systemctl daemon-reload: 因为我们添加了新的 service 文件。
   systemctl enable cri-dockerd.service: 设置 cri-dockerd 服务开机自启。
   systemctl start cri-dockerd.service: 启动 cri-dockerd 服务。
   systemctl status cri-dockerd.service: 检查服务是否 active (running)。
   第 III 部分：集成与验证
1. 配置 kubelet 使用 cri-dockerd
   解释: 这是最后一步，告诉 kubelet 去和 cri-dockerd 通信，而不是直接找 containerd 或其他运行时。
   动作: 在 kubelet 的启动参数中，必须指定：
   --container-runtime-endpoint=unix:///run/cri-dockerd.sock (这是 cri-dockerd 默认监听的 socket 路径)
   --cgroup-driver=systemd (再次强调，必须与 Docker 的 daemon.json 配置一致)
2. 验证 CRI 接口
   解释: 使用 crictl 工具验证 kubelet 将要使用的 CRI 端点是否正常工作。
   动作:
   crictl --runtime-endpoint unix:///run/cri-dockerd.sock info
   如果命令成功返回 Docker 的版本信息和 cri-dockerd 的信息，说明整个链路 (crictl -> cri-dockerd -> dockerd) 是通的。
   第 IV 部分：清理与卸载
1. 清理 cri-dockerd
   停止并禁用服务: systemctl stop cri-dockerd.service && systemctl disable cri-dockerd.service
   删除二进制文件: rm /usr/local/bin/cri-dockerd
   删除 service 文件: rm /etc/systemd/system/cri-dockerd.service /etc/systemd/system/cri-dockerd.socket
   重载 systemd: systemctl daemon-reload
2. 清理 Docker
   停止并禁用服务: systemctl stop docker && systemctl disable docker
   卸载软件包: apt-get purge docker-ce docker-ce-cli ... (使用 purge 会删除配置文件)
   删除所有数据 (危险操作): rm -rf /var/lib/docker 和 rm -rf /var/lib/containerd。这将删除所有镜像、容器和卷。
   总结：Docker + cri-dockerd 的步骤全吗？
   是的，以上步骤非常全面，覆盖了从安装、配置、集成到清理的全过程。
   关键要点回顾:
   两层结构: 需要同时安装和管理 Docker 和 cri-dockerd。
   Cgroup Driver 一致性: kubelet 和 dockerd 的 cgroup-driver 都必须是 systemd。这是最常见的失败原因。
   Kubelet 端点: kubelet 的 --container-runtime-endpoint 必须指向 cri-dockerd 的 socket (unix:///run/cri-dockerd.sock)。
   为什么要这么做: 这是在 K8s v1.24+ 版本中继续使用 Docker Engine 的唯一官方推荐方式。
   与 containerd 方案的对比:
   containerd 方案: 更简单、更原生。kubelet 直接与 containerd 的 CRI 插件通信，少了一个中间环节，理论上性能更好，维护也更简单。对于新集群，这是首选方案。
   docker + cri-dockerd 方案: 更复杂，多了一个需要维护的组件 (cri-dockerd)。主要优势在于可以继续使用熟悉的 docker CLI 命令进行调试，并且兼容依赖 Docker 的旧有 CI/CD 流程或脚本。