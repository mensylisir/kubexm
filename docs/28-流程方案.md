### **统一流程方案：从用户输入到集群就绪**

**核心组件概览**

- **kubexm CLI**: 用户交互的命令行工具。
- **kubexm-api-server**: (可选，用于自动化) 提供REST API的HTTP服务。
- **Job Database**: 一个简单的数据库（如PostgreSQL, SQLite, Redis），用于任务持久化。
    - jobs 表: id, cluster_config_yaml, status, current_step, logs, created_at, updated_at...
- **kubexm-worker**: 后台工作进程，真正执行部署任务。
- **kubexm-controller-manager**: （Operator模式）运行在目标集群中的控制器。

------



### **模式一：引导流程 (Bootstrap Mode)**

**目标**: 从零开始，在一组裸机/虚拟机上部署一个全新的Kubernetes集群。

**第 0 步: 用户发起创建请求**

- **路径 A (CLI)**: 用户在跳板机上执行 kubexm create cluster -f ./my-cluster.yaml。
- **路径 B (API)**: 自动化系统发送 POST /api/v1/clusters 请求，请求体为 my-cluster.yaml 的内容。

**第 1 步: 任务接收与持久化 (API/CLI -> App Service)**

1. **解析与验证**:
    - kubexm CLI或kubexm-api-server接收到请求。
    - 调用pkg/config加载器，尝试将YAML解析为v1alpha1.Cluster对象。**这步只做最基本的语法验证，确保YAML格式正确。**
2. **创建任务记录**:
    - 调用**应用服务层 (pkg/app)** 的 CreateBootstrapJob 函数。
    - 该函数连接到 **Task Database**。
    - 在 jobs 表中插入一条新记录：
        - 生成一个唯一的 job_id (e.g., UUID)。
        - cluster_config_yaml: 存储完整的 my-cluster.yaml 原文。
        - status: 设置为 Pending。
        - logs: 初始化为空。
3. **立即返回**:
    - CLI打印 Job created with ID: <task_id>. Run 'kubexm get job <job_id>' to check status.
    - API服务返回 202 Accepted，响应体为 { "job_id": "<job_id>" }。
    - **至此，用户的前台交互结束。**

**第 2 步: 任务执行 (Worker)**

1. **任务拾取**:
    - kubexm-worker 进程（可以与API服务在同一个docker-compose中，但逻辑独立）持续轮询 **Job Database**，查找 status 为 Pending 的任务。
    - 当它找到一个任务，它会用一个事务将该任务的status更新为 Processing，以防其他worker重复拾取。
2. **初始化执行环境 (Runtime)**:
    - Worker读取任务记录中的cluster_config_yaml。
    - 再次调用pkg/config，将其**正式反序列化**为内存中的v1alpha1.Cluster对象。
    - 创建pkg/runtime.Runtime实例，并将Cluster对象、Logger实例、以及一个**数据库连接器（用于更新任务状态）**注入其中。
    - **关键**: Runtime的初始化过程会并发地连接到spec.hosts中定义的所有主机，执行信息采集（Gather Facts），并将连接句柄和主机信息缓存起来。
3. **构建执行计划 (Pipeline -> Plan)**:
    - Worker调用顶层pkg/pipeline.NewCreateClusterPipeline()。
    - Pipeline、Module、Task层级开始工作，它们从传入的Runtime中读取配置 (Cluster对象) 和主机信息。
    - 每一层根据配置和逻辑，决策并构建出各自的**子图**。
    - 最终，Pipeline将所有子图组装成一个完整的、有向无环的**执行图 (ExecutionGraph)**，并将其存储在 pkg/plan.Plan 对象中。**这个图是静态的，描述了“要做什么”和“依赖关系”，但还未执行。**
4. **执行与状态反馈 (Engine)**:
    - Worker将 Plan 和 Runtime 交给 pkg/engine.Engine。
    - Engine开始**解释和调度**执行图：
        - 它根据图的拓扑结构，找到所有入度为0的节点（Step）。
        - 为每个可执行的Step启动一个goroutine。
        - 在每个goroutine中：
            - **a. 更新任务状态**: 执行前，通过Runtime中的数据库连接器，将tasks表中的current_step更新为当前Step的名称（如 "Install etcd on node-1"）。
            - **b. 执行Precheck**: 调用Step.Precheck()检查是否已满足期望状态。如果满足，跳到 d。
            - **c. 执行Run**: 调用Step.Run()执行实际操作。所有输出（stdout/stderr）通过Runtime中的Logger，附加task_id和step_name后，**实时写入tasks表的logs字段**。
            - **d. 报告结果**: Step执行完毕，Engine收集结果（成功/失败）。
        - 当一个Step成功后，Engine会更新图的状态，并检查其下游Step是否已满足执行条件，如果满足，则调度它们。
        - 这个过程持续进行，直到所有Step执行完毕或遇到不可忽略的错误。

**第 3 步: 任务完成与交接**

1. **收尾工作**:
    - Engine执行完毕，将最终的执行报告（包含所有Step的成功/失败状态）返回给Worker。
2. **更新最终状态**:
    - 如果所有Step成功，Worker将tasks表中对应任务的status更新为 Success。
    - 如果失败，则更新为 Failed，并将详细错误信息写入日志。
3. **交接 (Handoff)**:
    - 在CreateClusterPipeline的最后，会有两个特殊的Step:
        - InstallCRDStep: 连接到**新创建的集群**的APIServer，apply Cluster的CRD定义。
        - InstantiateCRStep: 将引导时使用的cluster.yaml内容，作为一个Cluster实例apply到新集群中。此时可以根据执行结果，为其填充初始的status字段。
    - **这一步完成了从“引导工具”到“集群内管理”的过渡准备。**

------



### **模式二：控制器流程 (Operator Mode)**

**目标**: 管理一个已存在的、由kubexm创建的集群，响应kubectl操作。

**第 0 步: 用户发起变更请求**

- 用户修改了本地的my-cluster.yaml（例如，将kubernetes.version从v1.25.3改为v1.26.1）。
- 执行 kubectl apply -f ./my-cluster.yaml。

**第 1 步: 事件触发 (Kubernetes API Server -> Controller)**

1. Kubernetes API Server接收到apply请求，更新Etcd中存储的Cluster对象。
2. API Server的事件机制通知kubexm-controller-manager：“default/my-cluster这个Cluster资源被更新了”。

**第 2 步: 协调循环 (Reconcile Loop in Controller)**

1. **获取对象**: 控制器的Reconcile函数被触发。它首先从API Server获取最新的Cluster对象，该对象包含用户的**新spec\**和控制器上次更新的\**旧status**。
2. **逻辑判断与差异分析**:
    - 控制器检查spec.kubernetes.version (v1.26.1) 与 status.kubernetes.version (v1.25.3)。
    - 它发现版本不一致，因此判断需要执行“升级”操作。
3. **构建执行计划 (Pipeline -> Plan)**:
    - **这一步与引导模式几乎完全相同！**
    - 控制器创建Runtime上下文，注入从API Server获取的Cluster对象。
    - 它调用一个专门的pkg/pipeline.NewUpgradeClusterPipeline()（或者一个更通用的、能处理所有变更的ReconcilePipeline）。
    - Pipeline及其下层组件根据spec和status的差异，**智能地只生成与“升级版本”相关的Step**，构成一个更小的、目标明确的ExecutionGraph。例如，它不会包含“初始化OS”或“安装Etcd”的步骤。
4. **执行 (Engine)**:
    - **这一步也与引导模式完全相同！**
    - 控制器将Plan和Runtime交给**同一个pkg/engine.Engine**。
    - Engine按照图的依赖关系，通过SSH连接到目标主机，执行升级操作（如替换二进制文件、重启服务等）。

**第 3 步: 状态更新**

1. Engine执行完毕，返回结果给控制器。
2. 控制器根据执行结果，构造一个新的status对象。例如：
    - 如果成功，它会更新status.kubernetes.version为v1.26.1，并更新Conditions字段，标记Type: Ready, Status: True。
    - 如果失败，它会记录错误信息到status.conditions中。
3. 控制器调用Kubernetes API，将这个新的status**写回到Cluster对象中**。

**至此，一个协调循环完成。集群的实际状态和status字段都已更新，与spec再次保持一致。**

这个详细的流程展示了您的架构如何优雅地统一两种模式，核心在于**将核心执行逻辑 (Pipeline到Engine) 与触发源（CLI/API vs. K8s Controller）彻底解耦**，并使用v1alpha1.Cluster作为它们之间不变的“通用语言”。